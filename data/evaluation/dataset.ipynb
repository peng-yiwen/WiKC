{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file Wiki_ET.txt\n",
    "PATH = '../wikidata/'\n",
    "if not os.path.exists(PATH+\"Wiki_ET.txt\"):\n",
    "    raise FileNotFoundError(\"Please first run postprocess.py in data_mining_scripts folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Wikidata and WiKC taxonomy\n",
    "oriWikiTaxonDown = defaultdict(set)\n",
    "with open(PATH+'wiki_taxonomy.tsv', 'r') as ori:\n",
    "    for line in ori:\n",
    "        triple = line.strip().split('\\t')\n",
    "        if len(triple) > 3:\n",
    "            oriWikiTaxonDown[triple[2]].add(triple[0])\n",
    "oriWiki = nx.DiGraph(oriWikiTaxonDown)\n",
    "\n",
    "cleanWikiTaxonDown = defaultdict(set)\n",
    "with open('../wikc.tsv', 'r') as clean:\n",
    "    for line in clean:\n",
    "        child, parent = line.strip().split('\\t')\n",
    "        cleanWikiTaxonDown[parent].add(child)\n",
    "wikc = nx.DiGraph(cleanWikiTaxonDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_wiki_ents_en = set()\n",
    "with open(os.path.join('../wikipedia', 'enwiki'), 'r') as file:\n",
    "    for line in file:\n",
    "        qid = line.strip().split(',')[1]\n",
    "        prefix_qid = 'wd:'+str(qid)\n",
    "        mapped_wiki_ents_en.add(prefix_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload Number of edges deleted:  10698\n"
     ]
    }
   ],
   "source": [
    "# reload edges deleted\n",
    "if not os.path.exists('edges_del.tsv'):\n",
    "    raise FileNotFoundError(\"Please first run clean.ipynb to get the edges_del.tsv file\")\n",
    "edges_del = set()\n",
    "with open('edges_del.tsv', 'r') as file:\n",
    "    for line in file:\n",
    "        parent, child = line.strip().split('\\t')\n",
    "        edges_del.add(tuple([parent, child]))\n",
    "print(\"Reload Number of edges deleted: \", len(edges_del))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "def get_ancestors(ori_graph, ancestors, cls, cur_graph, depth):\n",
    "    for sp in ori_graph.predecessors(cls):\n",
    "        # path should not include irrelevant edges\n",
    "        if tuple([sp, cls]) in edges_del:\n",
    "            continue\n",
    "        if cur_graph.has_node(sp):\n",
    "            ancestors.add(tuple([sp, depth]))\n",
    "            continue\n",
    "        get_ancestors(ori_graph, ancestors, sp, cur_graph, depth+1)\n",
    "\n",
    "def get_valid_first_ancestors(ori_graph, cls, cur_graph):\n",
    "    ancestors = set()\n",
    "    get_ancestors(ori_graph, ancestors, cls, cur_graph, depth=0)\n",
    "    return ancestors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* retype instances to WiKC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst2type = defaultdict(set)\n",
    "with open(PATH+'Wiki_ET.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if inst not in mapped_wiki_ents_en:\n",
    "            continue\n",
    "        # instance has a Wikipedia page\n",
    "        if wikc.has_node(cls):\n",
    "            inst2type[inst].add(cls)\n",
    "            continue\n",
    "        # No direct class exists anymore\n",
    "        fir_ancestors = list(get_valid_first_ancestors(oriWiki, cls, wikc))\n",
    "        sorted_ances = sorted(fir_ancestors, key=lambda x: x[1])\n",
    "        # only keep ancestors with the minimum depth\n",
    "        min_depth = 1e9\n",
    "        for tupl in sorted_ances:\n",
    "            ances, depth = tupl\n",
    "            if depth <= min_depth:\n",
    "                min_depth = depth\n",
    "                inst2type[inst].add(ances)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204533/7204533 [05:03<00:00, 23720.45it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "# remove transtive types for each instance\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120':\n",
    "            # it's pointless to retype to root class\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2):\n",
    "            # oriWiki -> some insts retype to top level classes being wrong\n",
    "            valcls.remove(node1)\n",
    "    inst2type[inst] = valcls.copy() # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204533/7204533 [00:05<00:00, 1257552.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# filtering out instances typed to class of top3lavel classes\n",
    "root = 'wd:Q35120'\n",
    "top3_cls = set(nx.single_source_shortest_path_length(wikc, source=root, cutoff=3).keys())\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    del_set = inst2type[inst].intersection(top3_cls)\n",
    "    if len(del_set) > 0:\n",
    "        for cls in del_set:\n",
    "            inst2type[inst].remove(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204533/7204533 [00:19<00:00, 372467.38it/s] \n"
     ]
    }
   ],
   "source": [
    "# check if empty set exists\n",
    "inst2cls = {}\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) > 0:\n",
    "        inst2cls[inst] = inst2type[inst].copy()\n",
    "# delete inst2type\n",
    "del inst2type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* extract a subset of instances for extrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cumulative stats is for instances of classes\n",
    "def getSuperClasses(cls, classes, WikiTaxonomyUp):\n",
    "    \"\"\"Adds all superclasses of a class <cls> (including <cls>) to the set <classes>\"\"\"\n",
    "    classes.add(cls)\n",
    "    # Make a check before because it's a defaultdict,\n",
    "    # which would create cls if it's not there\n",
    "    if cls in WikiTaxonomyUp:\n",
    "        for sc in WikiTaxonomyUp[cls]:\n",
    "            getSuperClasses(sc, classes, WikiTaxonomyUp)      \n",
    "\n",
    "def getAncestors(cls, WikiTaxonomyUp):\n",
    "    \"\"\"Returns the set of all parent classes of <cls> (including <cls>!)\"\"\"\n",
    "    classes=set()\n",
    "    getSuperClasses(cls, classes, WikiTaxonomyUp)        \n",
    "    return classes\n",
    "\n",
    "def cumulative_stats(oristats, topTaxonomyUp):\n",
    "    \"\"\"Cumulative statistics of classes\"\"\"\n",
    "    cum_stats = defaultdict(int)\n",
    "    for instantiated_cls in oristats.keys():\n",
    "        ancestors = getAncestors(instantiated_cls, topTaxonomyUp) # including cls itself\n",
    "        for ancestor in ancestors:\n",
    "            cum_stats[ancestor] += oristats[instantiated_cls]\n",
    "    return cum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cumulative number of instances for 'Person':  2575432\n"
     ]
    }
   ],
   "source": [
    "# check current cumulative number of instances for 'Person'\n",
    "from collections import defaultdict\n",
    "cls_stats = defaultdict(int)\n",
    "for inst in inst2cls.keys():\n",
    "    for cls in inst2cls[inst]:\n",
    "        cls_stats[cls] += 1\n",
    "cum_cls_insts = cumulative_stats(cls_stats, nx.to_dict_of_lists(wikc.reverse()))\n",
    "print(\"Current cumulative number of instances for 'Person': \", cum_cls_insts['wd:Q215627'])\n",
    "# too much instances for 'Person' class -> imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5019581/5019581 [00:05<00:00, 901315.51it/s] \n"
     ]
    }
   ],
   "source": [
    "cls_insts = defaultdict(set)\n",
    "for inst in tqdm(inst2cls.keys()):\n",
    "    for cls in inst2cls[inst]:\n",
    "        cls_insts[cls].add(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: limit each class less than 1000 instances\n",
    "import random\n",
    "for cls in cls_insts:\n",
    "    if len(cls_insts[cls]) > 100:\n",
    "        sampled_insts = random.sample(cls_insts[cls], 100)\n",
    "        cls_insts[cls] = set(sampled_insts).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13613/13613 [00:00<00:00, 41759.26it/s]\n"
     ]
    }
   ],
   "source": [
    "inst_cls = defaultdict(set)\n",
    "for cls in tqdm(cls_insts):\n",
    "    for inst in cls_insts[cls]:\n",
    "        inst_cls[inst].add(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: select at random 100k instances overall\n",
    "sample_keys = random.sample(list(inst_cls.keys()), 100000)\n",
    "sampled_inst2type = {key: inst_cls[key] for key in sample_keys} # wikc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances:  100000\n"
     ]
    }
   ],
   "source": [
    "# step3: classes for sampled instances in original Wikidata\n",
    "ori_inst2type = defaultdict(set)\n",
    "with open(PATH+'Wiki_ET.txt', 'r') as etreader:\n",
    "    for line in etreader:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if inst in sampled_inst2type:\n",
    "            ori_inst2type[inst].add(cls)\n",
    "\n",
    "# same as before: remove transtive types for each instance\n",
    "for inst in ori_inst2type.keys():\n",
    "    if len(ori_inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = ori_inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(ori_inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120':\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2):\n",
    "            valcls.remove(node1)\n",
    "    ori_inst2type[inst] = valcls.copy()\n",
    "\n",
    "print(\"Number of instances: \", len(ori_inst2type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels and descriptions for instances\n",
    "ent2label, ent2desc = {}, {}\n",
    "with open(PATH+'Wiki_literals.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        qid, rel, literal = line.strip().split('\\t')\n",
    "        if rel == 'rdfs:label':\n",
    "            ent2label[qid] = literal[1:-4]\n",
    "            continue\n",
    "        if rel == 'schema:description':\n",
    "            ent2desc[qid] = literal[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances with labels:  38400123\n",
      "Number of instances with descriptions:  38400123\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of instances with labels: \", len(ent2label))\n",
    "print(\"Number of instances with descriptions: \", len(ent2desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def get_parents_with_hops(graph, node):\n",
    "    '''\n",
    "    @param node: qid of wikidata entity\n",
    "    '''\n",
    "    # dictionary to store parents of node and their hop count\n",
    "    parents_with_hops = defaultdict(list) # {hop_count: [parents]}\n",
    "    # queue to perform BFS\n",
    "    queue = deque([(node, 0)])\n",
    "    visited = set()\n",
    "    \n",
    "    while queue:\n",
    "        current_node, hop_count = queue.popleft()\n",
    "        \n",
    "        if current_node not in visited:\n",
    "            visited.add(current_node)\n",
    "            \n",
    "            # For each predecessor (parent) of the current node\n",
    "            for parent in graph.predecessors(current_node):\n",
    "                if parent not in visited:\n",
    "                    parents_with_hops[hop_count + 1].append(parent)\n",
    "                    queue.append((parent, hop_count + 1))\n",
    "    \n",
    "    return parents_with_hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_with_depth(graph, node, cls_depth):\n",
    "    # Retrieve the distances for all superclasses of the given node\n",
    "    superclasses_with_distances = {}\n",
    "    def collect_superclasses(node):\n",
    "        for predecessor in graph.predecessors(node):\n",
    "            if predecessor in cls_depth:\n",
    "                superclasses_with_distances[predecessor] = cls_depth[predecessor]\n",
    "                collect_superclasses(predecessor)\n",
    "    \n",
    "    collect_superclasses(node)\n",
    "    \n",
    "    return superclasses_with_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:02<00:00, 44929.47it/s]\n"
     ]
    }
   ],
   "source": [
    "cls_depth = nx.single_source_shortest_path_length(wikc, root)\n",
    "with open('wikc_eval.txt', 'w') as writer:\n",
    "    for inst in tqdm(sampled_inst2type):\n",
    "        types = sampled_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(wikc, cls, cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4699.35it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_cls_depth = nx.single_source_shortest_path_length(oriWiki, root)\n",
    "with open('wikidata_eval.txt', 'w') as writer:\n",
    "    for inst in tqdm(ori_inst2type):\n",
    "        types = ori_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = ori_cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(oriWiki, cls, ori_cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a mini-test dataset: only 1k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_keys = random.sample(list(inst_cls.keys()), 1000)\n",
    "mini_sampled_inst2type = {key: inst_cls[key] for key in sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances:  1000\n"
     ]
    }
   ],
   "source": [
    "mini_ori_inst2type = defaultdict(set)\n",
    "with open(PATH+'Wiki_ET.txt', 'r') as etreader:\n",
    "    for line in etreader:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if inst in mini_sampled_inst2type:\n",
    "            mini_ori_inst2type[inst].add(cls)\n",
    "\n",
    "# same as before: remove transtive types for each instance\n",
    "for inst in mini_ori_inst2type.keys():\n",
    "    if len(mini_ori_inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = mini_ori_inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(mini_ori_inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120':\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2):\n",
    "            valcls.remove(node1)\n",
    "    mini_ori_inst2type[inst] = valcls.copy()\n",
    "\n",
    "print(\"Number of instances: \", len(mini_ori_inst2type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_depth = nx.single_source_shortest_path_length(wikc, 'wd:Q35120')\n",
    "with open('wikc_eval_1k.txt', 'w') as writer:\n",
    "    for inst in tqdm(mini_sampled_inst2type):\n",
    "        types = mini_sampled_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(wikc, cls, cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "ori_cls_depth = nx.single_source_shortest_path_length(oriWiki, 'wd:Q35120')\n",
    "with open('wikidata_eval_1k.txt', 'w') as writer:\n",
    "    for inst in tqdm(mini_ori_inst2type):\n",
    "        types = mini_ori_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = ori_cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(oriWiki, cls, ori_cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
