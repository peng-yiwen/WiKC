{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_path = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/dataset/Wiki_ET.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_wiki_ents_en = set()\n",
    "with open('../wikipedia/enwiki', 'r') as enwiki:\n",
    "    for line in enwiki:\n",
    "        qid = line.strip().split(',')[1]\n",
    "        mapped_wiki_ents_en.add('wd:'+str(qid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9784084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapped_wiki_ents_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Wiki_ET_subset.txt', 'w') as etwriter: # after enwiki filtering\n",
    "    with open(et_path, 'r') as et:\n",
    "        for line in et:\n",
    "            inst = line.strip().split('\\t')[0]\n",
    "            if inst in mapped_wiki_ents_en:\n",
    "                etwriter.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name: Wiki_ET_after_wikipedia_filtering.txt, file_size: 315975909, chunk_size: 2468561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6995458"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inst_stats = utils.subj_mentions('Wiki_ET_after_wikipedia_filtering.txt')\n",
    "# len(inst_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load ori / clean graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "ori_graph = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/wikidata_src/wiki_taxonomy.tsv'\n",
    "clean_graph = '/home/infres/ypeng-21/work/Taxon_clean/LLM/data/graph/clean_wikiTaxonUp.tsv'\n",
    "\n",
    "oriWikiTaxonDown = defaultdict(set)\n",
    "with open(ori_graph, 'r') as ori:\n",
    "    for line in ori:\n",
    "        triple = line.strip().split('\\t')\n",
    "        if len(triple) > 3:\n",
    "            oriWikiTaxonDown[triple[2]].add(triple[0])\n",
    "oriWiki = nx.DiGraph(oriWikiTaxonDown)\n",
    "\n",
    "cleanWikiTaxonDown = defaultdict(set)\n",
    "with open(clean_graph, 'r') as clean:\n",
    "    for line in clean:\n",
    "        child, parent = line.strip().split('\\t')\n",
    "        cleanWikiTaxonDown[parent].add(child)\n",
    "cleanWiki = nx.DiGraph(cleanWikiTaxonDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph: 3962959 4657644\n",
      "Cleaned graph: 16613 19654\n"
     ]
    }
   ],
   "source": [
    "print('Original graph:', oriWiki.number_of_nodes(), oriWiki.number_of_edges())\n",
    "print('Cleaned graph:', cleanWiki.number_of_nodes(), cleanWiki.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497890"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redundant taxonomic links: oriwiki\n",
    "oriWiki.number_of_edges() - nx.transitive_reduction(oriWiki).number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* avg path to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111397057/111397057 [06:30<00:00, 285220.68it/s]\n",
      "100%|██████████| 11128995/11128995 [00:22<00:00, 485081.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cls_stats = defaultdict(int)\n",
    "inst_cls_stats = defaultdict(int)\n",
    "inst_cls = defaultdict(set)\n",
    "\n",
    "path1 = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/inst_P31_cls.txt'\n",
    "path2 = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/occupation_P106.txt'\n",
    "\n",
    "with open(path1, 'r') as file:\n",
    "    for line in tqdm(file, total=111397057):\n",
    "        tupl = line.strip().split('\\t')\n",
    "        if len(tupl) < 2:\n",
    "            continue\n",
    "        else:\n",
    "            inst, cls = tupl\n",
    "            cls_stats[cls] += 1\n",
    "            inst_cls_stats[inst] += 1\n",
    "            inst_cls['wd:'+inst].add('wd:'+cls)\n",
    "\n",
    "with open(path2, 'r') as file:\n",
    "    for line in tqdm(file, total=11128995):\n",
    "        tupl = line.strip().split('\\t')\n",
    "        if len(tupl) < 2:\n",
    "            continue\n",
    "        else:\n",
    "            inst, cls = tupl\n",
    "            cls_stats[cls] += 1\n",
    "            inst_cls_stats[inst] += 1\n",
    "            inst_cls['wd:'+inst].add('wd:'+cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104836686/104836686 [16:59<00:00, 102846.45it/s]\n",
      "100%|██████████| 104277315/104277315 [8:58:10<00:00, 3229.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104277315/104277315 [01:58<00:00, 878049.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104277315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104277315/104277315 [08:44<00:00, 198848.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90580083\n"
     ]
    }
   ],
   "source": [
    "# retyping\n",
    "tmp = np.load('edges_del.npy')\n",
    "del_edges = set()  # tuple set\n",
    "for edge in tmp:\n",
    "    del_edges.add(tuple(edge))\n",
    "print(len(del_edges)) \n",
    "wikiTaxonUp = nx.to_dict_of_lists(cleanWiki.reverse())\n",
    "\n",
    "\n",
    "# Retyping\n",
    "from itertools import permutations\n",
    "global del_edges\n",
    "def get_ancestors(digraph, ancestors, cls, depth):\n",
    "    for sp in digraph.predecessors(cls):\n",
    "        # path should not include irrelevant edges\n",
    "        if tuple([sp, cls]) in del_edges:\n",
    "            continue\n",
    "        if sp in wikiTaxonUp:\n",
    "            ancestors.add(tuple([sp, depth]))\n",
    "            continue\n",
    "        get_ancestors(digraph, ancestors, sp, depth+1)\n",
    "\n",
    "def get_valid_first_ancestors(digraph, cls):\n",
    "    ancestors = set()\n",
    "    get_ancestors(digraph, ancestors, cls, depth=0)\n",
    "    return ancestors\n",
    "\n",
    "\n",
    "# Process Retype\n",
    "inst2type = defaultdict(set)\n",
    "for inst in tqdm(inst_cls.keys()):\n",
    "    cls_set = inst_cls[inst]\n",
    "    for cls in cls_set:\n",
    "        if cls in wikiTaxonUp:\n",
    "            # class avaliable in the clean graph\n",
    "            inst2type[inst].add(cls)\n",
    "            continue\n",
    "        if not oriWiki.has_node(cls):\n",
    "            continue\n",
    "        # No direct class exists anymore\n",
    "        fir_ancestors = list(get_valid_first_ancestors(oriWiki, cls))\n",
    "        sorted_ances = sorted(fir_ancestors, key=lambda x: x[1])\n",
    "        min_depth = 1e9\n",
    "        # retype to the first ancestor\n",
    "        for tupl in sorted_ances:\n",
    "            ances, depth = tupl\n",
    "            if depth <= min_depth:\n",
    "                min_depth = depth\n",
    "                inst2type[inst].add(ances)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "# remove transtive parents types for each instance\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120': # not retyped to 'entity'\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2): # !!!!clean or Ori?? 不用clean的原因为，retyped到的很高层次的type很可能是错的\n",
    "            valcls.remove(node1)\n",
    "    inst2type[inst] = valcls.copy() # set\n",
    "\n",
    "\n",
    "# filtering out instances typed to class of top3lavel classes\n",
    "top3_cls = set(nx.single_source_shortest_path_length(cleanWiki, source='wd:Q35120', cutoff=3).keys())\n",
    "print(len(top3_cls))\n",
    "\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    del_set = inst2type[inst].intersection(top3_cls)\n",
    "    if len(del_set) > 0:\n",
    "        for cls in del_set:\n",
    "            inst2type[inst].remove(cls)\n",
    "\n",
    "\n",
    "# check if empty set exists\n",
    "print(len(inst2type))\n",
    "clean_inst2type = {}\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) > 0:\n",
    "        clean_inst2type[inst] = inst2type[inst].copy()\n",
    "# delete inst2type\n",
    "del inst2type\n",
    "print(len(clean_inst2type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def count_paths_to_root(G, root, nodes_list):\n",
    "    path_counts = {}\n",
    "    for node in tqdm(nodes_list):\n",
    "        if node == root:\n",
    "            path_counts[node] = 0  # Only one path from root to itself\n",
    "        else:\n",
    "            paths = list(nx.all_simple_paths(G, node, root))\n",
    "            path_counts[node] = len(paths)\n",
    "    return path_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 61674934/90580083 [00:55<00:26, 1101647.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90580083/90580083 [01:22<00:00, 1096317.61it/s]\n"
     ]
    }
   ],
   "source": [
    "new_cls_stats = defaultdict(int)\n",
    "for inst in tqdm(clean_inst2type.keys()):\n",
    "    for cls in clean_inst2type[inst]:\n",
    "        new_cls_stats[cls] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15881/15881 [00:00<00:00, 33062.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of paths to the root wd:Q35120: 2.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = 'wd:Q35120'\n",
    "cls_list = list(new_cls_stats.keys())\n",
    "path_counts = count_paths_to_root(cleanWiki.reverse(), root, cls_list)\n",
    "\n",
    "total_inst_path_count = 0\n",
    "for cls, count in path_counts.items():\n",
    "    total_inst_path_count += count * new_cls_stats[cls]\n",
    "\n",
    "average_paths = total_inst_path_count / sum(new_cls_stats.values())\n",
    "print(f\"Average number of paths to the root {root}: {average_paths:.2f}\")\n",
    "\n",
    "# 2.56 if don't remove transitive links\n",
    "# 2.85 if remove transitive links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes without cumulative instances 3880217\n"
     ]
    }
   ],
   "source": [
    "# Classes without cumulative instances: oriwikis, strict as we set a lot of constraints on instances\n",
    "path = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/dataset/cum_cls_inst_stats.txt'\n",
    "cls_with_insts = set()\n",
    "with open(path, 'r') as file:\n",
    "    for line in file:\n",
    "        cls, num = line.strip().split('\\t')\n",
    "        cls_with_insts.add(cls)\n",
    "print(\"Classes without cumulative instances\", oriWiki.number_of_nodes() - len(cls_with_insts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is DAG? True\n",
      "Is connected? True\n",
      "Max Depth: 19\n",
      "Top-level classes 38\n"
     ]
    }
   ],
   "source": [
    "print(\"Is DAG?\", nx.is_directed_acyclic_graph(oriWiki))\n",
    "print(\"Is connected?\", nx.is_weakly_connected(oriWiki))\n",
    "print(\"Max Depth:\", max(nx.shortest_path_length(oriWiki, source='wd:Q35120').values()))\n",
    "print(\"Top-level classes\", oriWiki.out_degree('wd:Q35120'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is DAG? True\n",
      "Is connected? True\n",
      "Max Depth: 13\n",
      "Top-level classes 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Is DAG?\", nx.is_directed_acyclic_graph(cleanWiki))\n",
    "print(\"Is connected?\", nx.is_weakly_connected(cleanWiki))\n",
    "print(\"Max Depth:\", max(nx.shortest_path_length(cleanWiki, source='wd:Q35120').values()))\n",
    "print(\"Top-level classes\", cleanWiki.out_degree('wd:Q35120'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Prefixes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "\n",
    "def prop_mentions(file_path):\n",
    "    return parallel_read(file_path, count_properties)\n",
    "\n",
    "\n",
    "def ent_mentions(file_path):\n",
    "    return parallel_read(file_path, count_entities)\n",
    "\n",
    "\n",
    "def cls_mentions(file_path):\n",
    "    return parallel_read(file_path, count_class_insts)\n",
    "\n",
    "\n",
    "def inst_type_mentions(file_path):\n",
    "    return parallel_read(file_path, count_inst_types)\n",
    "\n",
    "\n",
    "def subj_mentions(file_path):\n",
    "    return parallel_read(file_path, count_subj)\n",
    "\n",
    "\n",
    "def process_chunk(file_name, chunk_start, chunk_end, func):\n",
    "    # chunk_results = []\n",
    "    with open(file_name, 'r', encoding='UTF-8') as f:\n",
    "        # Moving stream position to `chunk_start`\n",
    "        f.seek(chunk_start)\n",
    "\n",
    "        # Read and process lines until `chunk_end`\n",
    "        chunk_results = defaultdict(int)\n",
    "        for line in f:\n",
    "            chunk_start += len(line)\n",
    "            if chunk_start > chunk_end:\n",
    "                break\n",
    "\n",
    "            elms = func(line)\n",
    "            if len(elms) > 0:\n",
    "                for e in elms:\n",
    "                    chunk_results[e] += 1\n",
    "    return chunk_results\n",
    "\n",
    "\n",
    "def parallel_read(file_name, func):\n",
    "    # Maximum number of processes we can run at a time\n",
    "    cpu_count = mp.cpu_count()\n",
    "\n",
    "    file_size = os.path.getsize(file_name)\n",
    "    chunk_size = np.floor(file_size / cpu_count).astype(int)\n",
    "    print(f'file_name: {file_name}, file_size: {file_size}, chunk_size: {chunk_size}')\n",
    "\n",
    "    # Arguments for each chunk (eg. [('input.txt', 0, 32), ('input.txt', 32, 64)])\n",
    "    chunk_args = []\n",
    "    with open(file_name, 'rb') as f: # 'rb' avoding utf-8 decoding problems\n",
    "        # def is_start_of_line(position):\n",
    "        #     if position == 0:\n",
    "        #         return True\n",
    "        #     # Check whether the previous character is EOL\n",
    "        #     f.seek(position - 1)\n",
    "        #     return f.read(1) == '\\n'\n",
    "\n",
    "        def get_next_line_position(position):\n",
    "            # Read the current line till the end\n",
    "            f.seek(position)\n",
    "            f.readline()\n",
    "            # Return a position after reading the line\n",
    "            return f.tell()\n",
    "\n",
    "        chunk_start = 0\n",
    "        # Iterate over all chunks and construct arguments for `process_chunk`\n",
    "        while chunk_start < file_size:\n",
    "            chunk_end = min(file_size, chunk_start + chunk_size)\n",
    "\n",
    "            # # Make sure the chunk ends at the beginning of the next line\n",
    "            # while not is_start_of_line(chunk_end):\n",
    "            #     chunk_end -= 1\n",
    "\n",
    "            # Handle the case when a line is too long to fit the chunk size\n",
    "            # if chunk_start == chunk_end:\n",
    "            chunk_end = get_next_line_position(chunk_end)\n",
    "\n",
    "            # Save `process_chunk` arguments\n",
    "            args = (file_name, chunk_start, chunk_end, func)\n",
    "            chunk_args.append(args)\n",
    "\n",
    "            # Move to the next chunk\n",
    "            chunk_start = chunk_end\n",
    "\n",
    "    with mp.Pool(cpu_count) as p:\n",
    "        # Run chunks in parallel\n",
    "        chunk_results = p.starmap(process_chunk, chunk_args)\n",
    "\n",
    "    results = defaultdict(int)\n",
    "    # Combine chunk results into `results`\n",
    "    for chunk_result in chunk_results:\n",
    "        for r in chunk_result.keys():\n",
    "            results[r] += chunk_result[r]\n",
    "    return results\n",
    "\n",
    "\n",
    "def count_properties(line):\n",
    "    if not line.startswith(\"#\") and not line.startswith(\"@\"):\n",
    "        triples = line.rstrip().split(\"\\t\")\n",
    "        if len(triples)>2:\n",
    "            return [triples[1]]\n",
    "    return []\n",
    "\n",
    "\n",
    "def count_entities(line):\n",
    "    if not line.startswith(\"#\") and not line.startswith(\"@\"):\n",
    "        triples = line.rstrip().split(\"\\t\")\n",
    "        if len(triples)>2:\n",
    "            return [triples[0], triples[2]]\n",
    "    return []\n",
    "\n",
    "\n",
    "def count_class_insts(line):\n",
    "    if not line.startswith(\"#\") and not line.startswith(\"@\"):\n",
    "        triples = line.rstrip().split(\"\\t\")\n",
    "        if len(triples)>2 and triples[1] == Prefixes.rdfType:\n",
    "            return [triples[2]] # class information\n",
    "    return []\n",
    "\n",
    "\n",
    "def count_inst_types(line):\n",
    "    if not line.startswith(\"#\") and not line.startswith(\"@\"):\n",
    "        triples = line.rstrip().split(\"\\t\")\n",
    "        if len(triples)>2 and triples[1] == Prefixes.rdfType:\n",
    "            return [triples[0]] # instance information\n",
    "    return []\n",
    "\n",
    "\n",
    "def count_subj(line):\n",
    "    if not line.startswith(\"#\") and not line.startswith(\"@\"):\n",
    "        triples = line.rstrip().split(\"\\t\")\n",
    "        if len(triples)>2:\n",
    "            return [triples[0]]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "cls_stats = defaultdict(int)\n",
    "path = '/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/dataset/Wiki_ET.txt'\n",
    "with open(path, 'r') as file:\n",
    "    for line in file:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        cls_stats[cls] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Retyping the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10695\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tmp = np.load('edges_del.npy')\n",
    "del_edges = set()  # tuple set\n",
    "for edge in tmp:\n",
    "    del_edges.add(tuple(edge))\n",
    "print(len(del_edges)) \n",
    "wikiTaxonUp = nx.to_dict_of_lists(cleanWiki.reverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retyping\n",
    "from itertools import permutations\n",
    "global del_edges\n",
    "def get_ancestors(digraph, ancestors, cls, depth):\n",
    "    for sp in digraph.predecessors(cls):\n",
    "        # path should not include irrelevant edges\n",
    "        if tuple([sp, cls]) in del_edges:\n",
    "            continue\n",
    "        if sp in wikiTaxonUp:\n",
    "            ancestors.add(tuple([sp, depth]))\n",
    "            continue\n",
    "        get_ancestors(digraph, ancestors, sp, depth+1)\n",
    "\n",
    "def get_valid_first_ancestors(digraph, cls):\n",
    "    ancestors = set()\n",
    "    get_ancestors(digraph, ancestors, cls, depth=0)\n",
    "    return ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd:Q35120 in wikiTaxonUp\n"
     ]
    }
   ],
   "source": [
    "if 'wd:Q35120' in wikiTaxonUp:\n",
    "    print('wd:Q35120 in wikiTaxonUp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Retype\n",
    "from tqdm import tqdm\n",
    "inst2type = defaultdict(set)\n",
    "with open('Wiki_ET_subset.txt', 'r') as et_file:\n",
    "    for line in et_file:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if cls in wikiTaxonUp:\n",
    "            inst2type[inst].add(cls)\n",
    "            continue\n",
    "        # No direct class exists anymore\n",
    "        fir_ancestors = list(get_valid_first_ancestors(oriWiki, cls))\n",
    "        sorted_ances = sorted(fir_ancestors, key=lambda x: x[1])\n",
    "        min_depth = 1e9\n",
    "        for tupl in sorted_ances:\n",
    "            ances, depth = tupl\n",
    "            if depth <= min_depth:\n",
    "                min_depth = depth\n",
    "                inst2type[inst].add(ances)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some leaves classes doesn't have an instance as here we do wikipedia filtering, so some classes may not have instances anymore\n",
    "if 'wd:Q85946972' in inst2type:\n",
    "    print(inst2type['wd:Q85946972'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7204396 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204396/7204396 [06:10<00:00, 19447.81it/s] \n"
     ]
    }
   ],
   "source": [
    "# remove transtive parents types for each instance\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120': # not retyped to 'entity'\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2): # !!!!clean or Ori?? 不用clean的原因为，retyped到的很高层次的type很可能是错的\n",
    "            valcls.remove(node1)\n",
    "    inst2type[inst] = valcls.copy() # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wd:Q1258, wd:Q884142\n",
    "inst2type['wd:Q1258']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204396/7204396 [00:05<00:00, 1215826.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# filtering out instances typed to class of top3lavel classes\n",
    "top3_cls = set(nx.single_source_shortest_path_length(cleanWiki, source='wd:Q35120', cutoff=3).keys())\n",
    "print(len(top3_cls))\n",
    "\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    del_set = inst2type[inst].intersection(top3_cls)\n",
    "    if len(del_set) > 0:\n",
    "        for cls in del_set:\n",
    "            inst2type[inst].remove(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7204396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7204396/7204396 [00:22<00:00, 318723.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5019756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check if empty set exists\n",
    "print(len(inst2type))\n",
    "clean_inst2type = {}\n",
    "for inst in tqdm(inst2type.keys()):\n",
    "    if len(inst2type[inst]) > 0:\n",
    "        clean_inst2type[inst] = inst2type[inst].copy()\n",
    "# delete inst2type\n",
    "del inst2type\n",
    "print(len(clean_inst2type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wd:Q79529'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inst2type['wd:Q1258']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5019756/5019756 [00:05<00:00, 889845.02it/s]\n"
     ]
    }
   ],
   "source": [
    "cls_insts = defaultdict(set)\n",
    "for inst in tqdm(clean_inst2type.keys()):\n",
    "    for cls in clean_inst2type[inst]:\n",
    "        cls_insts[cls].add(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13633"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_insts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cumulative stats is for instances of classes\n",
    "def getSuperClasses(cls, classes, WikiTaxonomyUp):\n",
    "    \"\"\"Adds all superclasses of a class <cls> (including <cls>) to the set <classes>\"\"\"\n",
    "    classes.add(cls)\n",
    "    # Make a check before because it's a defaultdict,\n",
    "    # which would create cls if it's not there\n",
    "    if cls in WikiTaxonomyUp:\n",
    "        for sc in WikiTaxonomyUp[cls]:\n",
    "            getSuperClasses(sc, classes, WikiTaxonomyUp)      \n",
    "\n",
    "def getAncestors(cls, WikiTaxonomyUp):\n",
    "    \"\"\"Returns the set of all parent classes of <cls> (including <cls>!)\"\"\"\n",
    "    classes=set()\n",
    "    getSuperClasses(cls, classes, WikiTaxonomyUp)        \n",
    "    return classes\n",
    "\n",
    "def cumulative_stats(oristats, topTaxonomyUp):\n",
    "    \"\"\"Cumulative statistics of classes\"\"\"\n",
    "    cum_stats = defaultdict(int)\n",
    "    for instantiated_cls in oristats.keys():\n",
    "        ancestors = getAncestors(instantiated_cls, topTaxonomyUp) # including cls itself\n",
    "        for ancestor in ancestors:\n",
    "            cum_stats[ancestor] += oristats[instantiated_cls]\n",
    "    return cum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cumulative stats\n",
    "from collections import defaultdict\n",
    "cls_stats = defaultdict(int)\n",
    "for cls in cls_insts.keys():\n",
    "    cls_stats[cls] = len(cls_insts[cls])\n",
    "\n",
    "cum_cls_insts = cumulative_stats(cls_stats, wikiTaxonUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2575434\n"
     ]
    }
   ],
   "source": [
    "if 'wd:Q215627' in cum_cls_insts:\n",
    "    print(cum_cls_insts['wd:Q215627'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3: limit each class less than 1000 instances\n",
    "import random\n",
    "for cls in cls_insts:\n",
    "    if len(cls_insts[cls]) > 100:\n",
    "        sampled_insts = random.sample(cls_insts[cls], 100)\n",
    "        cls_insts[cls] = set(sampled_insts).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13633/13633 [00:00<00:00, 50328.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inst_cls = defaultdict(set)\n",
    "for cls in tqdm(cls_insts):\n",
    "    for inst in cls_insts[cls]:\n",
    "        inst_cls[inst].add(cls)\n",
    "print(len(inst_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(path):\n",
    "    cls2label = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            # wd:Q96196524 rdfs:label \"current entity\" .\n",
    "            triple = line.strip().split('\\t')\n",
    "            if len(triple) > 3:\n",
    "                cls2label[triple[0]] = triple[2][1:-1]\n",
    "    return cls2label\n",
    "\n",
    "cls2label = load_label('/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/wikidata_src/wiki_taxonomy_labels.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* step4: select at random 100k instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_keys = random.sample(list(inst_cls.keys()), 50000)\n",
    "sampled_inst2type = {key: inst_cls[key] for key in sample_keys}\n",
    "print(len(sampled_inst2type))\n",
    "\n",
    "ori_inst2type = defaultdict(set)\n",
    "with open('Wiki_ET_subset.txt', 'r') as etreader:\n",
    "    for line in etreader:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if inst in sampled_inst2type:\n",
    "            ori_inst2type[inst].add(cls)\n",
    "print(len(ori_inst2type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:05<00:00, 9943.69it/s] \n"
     ]
    }
   ],
   "source": [
    "# remove transtive parents types for each instance for ori\n",
    "for inst in tqdm(ori_inst2type.keys()):\n",
    "    if len(ori_inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = ori_inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(ori_inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120': # not retyped to 'entity'\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2): # !!!!clean or Ori?? 不用clean的原因为，retyped到的很高层次的type很可能是错的\n",
    "            valcls.remove(node1)\n",
    "    ori_inst2type[inst] = valcls.copy() # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2label = {}\n",
    "ent2desc = {}\n",
    "with open('/home/infres/ypeng-21/work/Taxon_clean/build_dataset/test/build_data_from_nt/24_03_22/raw_data/dataset/Wiki_literals.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        qid, rel, literal = line.strip().split('\\t')\n",
    "        if rel == Prefixes.rdfsLabel:\n",
    "            ent2label[qid] = literal[1:-4]\n",
    "            continue\n",
    "        if rel == Prefixes.schemaDescription:\n",
    "            ent2desc[qid] = literal[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "def get_parents_with_hops(graph, node):\n",
    "    # node: qids\n",
    "    # dictionary to store parents of node and their hop count\n",
    "    parents_with_hops = defaultdict(list) # {hop_count: [parents]}\n",
    "    # queue to perform BFS\n",
    "    queue = deque([(node, 0)])\n",
    "    visited = set()\n",
    "    \n",
    "    while queue:\n",
    "        current_node, hop_count = queue.popleft()\n",
    "        \n",
    "        if current_node not in visited:\n",
    "            visited.add(current_node)\n",
    "            \n",
    "            # For each predecessor (parent) of the current node\n",
    "            for parent in graph.predecessors(current_node):\n",
    "                if parent not in visited:\n",
    "                    parents_with_hops[hop_count + 1].append(parent)\n",
    "                    queue.append((parent, hop_count + 1))\n",
    "    \n",
    "    return parents_with_hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def get_parents_with_depth(graph, node, cls_depth):\n",
    "    # Retrieve the distances for all superclasses of the given node\n",
    "    superclasses_with_distances = {}\n",
    "    def collect_superclasses(node):\n",
    "        for predecessor in graph.predecessors(node):\n",
    "            if predecessor in cls_depth:\n",
    "                superclasses_with_distances[predecessor] = cls_depth[predecessor]\n",
    "                collect_superclasses(predecessor)\n",
    "    \n",
    "    collect_superclasses(node)\n",
    "    \n",
    "    return superclasses_with_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write to file (as a eval dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 42067.46it/s]\n"
     ]
    }
   ],
   "source": [
    "cls_depth = nx.single_source_shortest_path_length(cleanWiki, 'wd:Q35120')\n",
    "with open('cleanWiki_eval.txt', 'w') as writer:\n",
    "    for inst in tqdm(sampled_inst2type):\n",
    "        types = sampled_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(cleanWiki, cls, cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wd:Q3624078', 'wd:Q43702', 'wd:Q185441'}\n",
      "{'wd:Q113489728', 'wd:Q3624078', 'wd:Q43702', 'wd:Q185441'}\n"
     ]
    }
   ],
   "source": [
    "if 'wd:Q31' in sampled_inst2type:\n",
    "    print(sampled_inst2type['wd:Q31'])\n",
    "    print(ori_inst2type['wd:Q31'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:10<00:00, 4778.87it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_cls_depth = nx.single_source_shortest_path_length(oriWiki, 'wd:Q35120')\n",
    "with open('oriWiki_eval.txt', 'w') as writer:\n",
    "    for inst in tqdm(ori_inst2type):\n",
    "        types = ori_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = ori_cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(oriWiki, cls, ori_cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sample only 5k samples as a test! -> additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_keys = random.sample(list(inst_cls.keys()), 1000)\n",
    "sampled_inst2type = {key: inst_cls[key] for key in sample_keys}\n",
    "print(len(sampled_inst2type))\n",
    "\n",
    "ori_inst2type = defaultdict(set)\n",
    "with open('Wiki_ET_subset.txt', 'r') as etreader:\n",
    "    for line in etreader:\n",
    "        inst, rel, cls = line.strip().split('\\t')\n",
    "        if inst in sampled_inst2type:\n",
    "            ori_inst2type[inst].add(cls)\n",
    "print(len(ori_inst2type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 10656.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove transtive parents types for each instance for ori\n",
    "for inst in tqdm(ori_inst2type.keys()):\n",
    "    if len(ori_inst2type[inst]) == 1:\n",
    "        continue\n",
    "    valcls = ori_inst2type[inst].copy()\n",
    "    for (node1,node2) in permutations(list(ori_inst2type[inst]), 2):\n",
    "        if node1 not in valcls or node2 not in valcls:\n",
    "            continue\n",
    "        if node1 == 'wd:Q35120': # not retyped to 'entity'\n",
    "            valcls.remove(node1)\n",
    "            continue\n",
    "        if nx.has_path(oriWiki, node1, node2): # !!!!clean or Ori?? 不用clean的原因为，retyped到的很高层次的type很可能是错的\n",
    "            valcls.remove(node1)\n",
    "    ori_inst2type[inst] = valcls.copy() # set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 16403.29it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4145.64it/s]\n"
     ]
    }
   ],
   "source": [
    "cls_depth = nx.single_source_shortest_path_length(cleanWiki, 'wd:Q35120')\n",
    "with open('cleanWiki_eval_1k.txt', 'w') as writer:\n",
    "    for inst in tqdm(sampled_inst2type):\n",
    "        types = sampled_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(cleanWiki, cls, cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "ori_cls_depth = nx.single_source_shortest_path_length(oriWiki, 'wd:Q35120')\n",
    "with open('oriWiki_eval_1k.txt', 'w') as writer:\n",
    "    for inst in tqdm(ori_inst2type):\n",
    "        types = ori_inst2type[inst]\n",
    "        parents_depth = {}\n",
    "        for cls in types:\n",
    "            parents_depth[cls] = ori_cls_depth[cls]\n",
    "            pars_with_dists = get_parents_with_depth(oriWiki, cls, ori_cls_depth)\n",
    "            parents_depth.update(pars_with_dists)\n",
    "        \n",
    "        for cls, depth in parents_depth.items():\n",
    "            writer.write(f\"{inst}\\t'{ent2label[inst]}'\\t'{ent2desc[inst]}'\\t{cls}\\t{depth}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(input_file, output_prefix, rows_per_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_files = len(lines) // rows_per_file + (1 if len(lines) % rows_per_file != 0 else 0)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        start_idx = i * rows_per_file\n",
    "        end_idx = min((i + 1) * rows_per_file, len(lines))\n",
    "        output_file = f\"{output_prefix}_{i + 1}.txt\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.writelines(lines[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'oriWiki_eval.txt'\n",
    "output_prefix = 'oriWiki_eval'\n",
    "rows_per_file = 210000\n",
    "split_file(input_file, output_prefix, rows_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
